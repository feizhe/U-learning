{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0393d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import Input, Model\n",
    "from joblib import Parallel, delayed, parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fb751",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"'drive/MyDrive/'\"\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, it = 1,\n",
    "                  p=50, err = 0.5,\n",
    "                  a=1, b=2, ## X ~ Unif[a,b]\n",
    "                  columns=[0, 5, 10, 15]):\n",
    "\n",
    "    # Simulate data\n",
    "    np.random.seed(it)\n",
    "    \n",
    "    # Generate 100 iid variables following uniform(a, b) distribution\n",
    "    X = np.random.uniform(low=a, high=b, size=(n, p))\n",
    "\n",
    "    X_selected = X[:, columns]\n",
    "\n",
    "    # Scenario 1 in paper\n",
    "    y = -np.sin(np.pi * X_selected[:, 0]) + 2 * (X_selected[:, 1] - 0.5)**2 + 1.5 * X_selected[:, 2] * X_selected[:, 3] - 5 / X_selected[:, 4]\n",
    "\n",
    "    # Add some noise to y\n",
    "    y_obs = y + np.random.normal(0, err, size=y.shape)\n",
    "\n",
    "    df = np.c_[y, y_obs, X]\n",
    "\n",
    "    fname = path + 'df_' + str(it) + '.txt'\n",
    "    np.savetxt(fname, df, delimiter='\\t')\n",
    "    \n",
    "    return X, y, y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=400\n",
    "ntest = 100\n",
    "p = 100\n",
    "X_test,y_test, y_test_obs = generate_data(n = ntest, it = 999, p= p)\n",
    "nsim = 101\n",
    "Brep = 102\n",
    "y_preds_arr = np.empty((nsim, Brep, n+ntest))\n",
    "ntrain_epochs = np.empty((nsim, Brep))\n",
    "val_losses = np.empty((nsim, Brep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_split(X, y, X_test, y_test, # model,\n",
    "              it = 1, b = 1, \n",
    "              th_val_loss = 0.3, \n",
    "              test_size=0.5,\n",
    "              model_dense_layer_1_activation='relu', \n",
    "              model_dense_layer_2_activation='relu',\n",
    "              early_stop_patience = 100, early_stop_min_delta = 0.001, \n",
    "              model_epochs=200, model_batch_size=32, model_loss='mse',\n",
    "              model_optimizer='adam', \n",
    "              verbose=False,\n",
    "              plot_res = False,\n",
    "              del_weights = True):\n",
    "\n",
    "    p = X.shape[1]\n",
    "    n = len(y)\n",
    "    ntest = len(y_test)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    indices = range(n)\n",
    "\n",
    "    X_train, X_val, y_train, y_val, indices_train, indices_val = train_test_split(X, y, indices,\n",
    "                                                                                  test_size = test_size,\n",
    "                                                                                  random_state = 1000*it+b) \n",
    "\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', min_delta=early_stop_min_delta, \n",
    "                       patience=early_stop_patience, \n",
    "                       verbose=0, mode='min',\n",
    "                       restore_best_weights = True)\n",
    "    \n",
    "\n",
    "    # Define and train the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation=model_dense_layer_1_activation, input_shape=(p,)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation=model_dense_layer_2_activation))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                        epochs=model_epochs, batch_size=32,\n",
    "                        callbacks=[es],  verbose = verbose)\n",
    "    \n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    train_loss = min(history.history['loss'])\n",
    "    train_epochs = len(history.history['val_loss'])\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    y_vec = np.zeros(n+ntest)\n",
    "    y_vec[indices_val] = y_val_pred.flatten()\n",
    "    y_vec[-ntest:] = y_pred.flatten()\n",
    "    \n",
    "    # if plot_res:\n",
    "    #     # Plot the training and validation errors versus epochs\n",
    "    #     plt.plot(history.history['loss'], label='Training Error')\n",
    "    #     plt.plot(history.history['val_loss'], label='Validation Error')\n",
    "    #     plt.xlabel('Epochs')\n",
    "    #     plt.ylabel('Error')\n",
    "    #     plt.legend()\n",
    "\n",
    "    #     # Save the plot to a file\n",
    "    #     plt.savefig('training_errs.png')\n",
    "    #     plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    return train_loss, val_loss, y_vec, train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4babcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(nsim):\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(it)\n",
    "    # create_folder(it)\n",
    "    X, y, y_obs = generate_data(n, it)\n",
    "    \n",
    "    n_epochs = []\n",
    "    val_loss_vec = []\n",
    "    y_mat = np.zeros((0, n+ntest))\n",
    "    for b in range(Brep):\n",
    "\n",
    "        val_loss, y_vec, train_epochs = one_split(X, y_obs, X_test, y_test, it = it, b = b)                        \n",
    "        print(train_epochs)\n",
    "        # Separate val_loss and y_vec into two separate lists\n",
    "        # val_losses = [result[0] for result in results]\n",
    "        # y_vecs = [result[1] for result in results]\n",
    "\n",
    "        val_loss_vec.append(val_loss)\n",
    "        n_epochs.append(train_epochs)\n",
    "        y_mat = np.vstack((y_mat, y_vec ) ) \n",
    "\n",
    "    # y_mat = np.array(y_vecs)\n",
    "    print(f\"y mat shape is {y_mat.shape}\")\n",
    "    \n",
    "    y_preds_arr[it, :, :] = y_mat\n",
    "    val_losses[it, :] = val_loss_vec\n",
    "    ntrain_epochs[it, :] = n_epochs\n",
    "\n",
    "    if it % 10 == 1:\n",
    "        \n",
    "        print(val_losses[it, :])\n",
    "        print(np.mean(ntrain_epochs, axis=1))\n",
    "\n",
    "        np.save(path+'_val_losses.npy', val_losses)\n",
    "        np.save(path+'_ntrain_epochs.npy', ntrain_epochs)\n",
    "        \n",
    "        np.save(path+'_y_preds_arr.npy', y_preds_arr)\n",
    "\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
